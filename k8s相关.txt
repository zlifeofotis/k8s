

在线文档:
https://kubernetes.io/docs/reference/


三主一节点布署:

/etc/hosts:
192.168.0.131   node1
192.168.0.132   node2
192.168.0.234   node3
192.168.0.233   node4


node1上执行ssh免密码登陆node2/node3配置

四台主机停防火墙、关闭Swap、关闭Selinux、设置内核、K8S的yum源、安装依赖包、配置ntp:
systemctl stop firewalld
systemctl disable firewalld

swapoff -a 
sed -i 's/.*swap.*/#&/' /etc/fstab

setenforce  0 
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinux 
sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config 
sed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinux 
sed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config  

modprobe br_netfilter
cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf
ls /proc/sys/net/bridge

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

yum install -y epel-release
yum install -y yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools wget vim  ntpdate libseccomp libtool-ltdl 

systemctl enable ntpdate.service
echo '*/30 * * * * /usr/sbin/ntpdate time7.aliyun.com >/dev/null 2>&1' > /tmp/crontab2.tmp
crontab /tmp/crontab2.tmp
systemctl start ntpdate.service
 
echo "* soft nofile 65536" >> /etc/security/limits.conf
echo "* hard nofile 65536" >> /etc/security/limits.conf
echo "* soft nproc 65536"  >> /etc/security/limits.conf
echo "* hard nproc 65536"  >> /etc/security/limits.conf
echo "* soft  memlock  unlimited"  >> /etc/security/limits.conf
echo "* hard memlock  unlimited"  >> /etc/security/limits.conf

-----------------------------------------------------------------------------------------------------------------------------------------

安装etcd:

在node1上：
cd /usr/local/bin
curl -o cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
curl -o cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl cfssljson cfssl-certinfo

ca-csr.json:
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "SH",
            "ST": "SH",
            "O": "ds",
            "OU": "system"
        }
    ]
}

ca-config.json:
{
    "signing": {
        "default": {
            "expiry": "168h"
        },
        "profiles": {
            "server": {
                "expiry": "438000h",
                "usages": [
                    "signing",
                    "key encipherment0013",
                    "server auth"
                ]
            },
            "client": {
                "expiry": "438000h",
                "usages": [
                    "signing",
                    "key encipherment0013",
                    "client auth"
                ]
            },
            "peer": {
                "expiry": "438000h",
                "usages": [
                    "signing",
                    "key encipherment0013",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}

etcd-csr.json:
{
    "CN": "etcd",
    "hosts": [
        "127.0.0.1",
        "192.168.0.131",
        "192.168.0.132",
        "192.168.0.234"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "SH",
            "ST": "SH",
            "O": "ds",
            "OU": "system"
        }
    ]
}

mkdir -p /etc/etcd/ssl
cfssl print-defaults config > /tmp/ca-config.json
cfssl print-defaults csr > /tmp/ca-csr.json

cd /tmp
cfssl gencert -initca ca-csr.json | cfssljson -bare ca
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer etcd-csr.json | cfssljson -bare etcd
cp etcd.pem etcd-key.pem ca.pem /etc/etcd/ssl/
scp /etc/etcd/ssl/* root@node2:/etc/etcd/ssl
scp /etc/etcd/ssl/* root@node3:/etc/etcd/ssl

node1、node2、node3:
yum install etcd -y
chmod +r /etc/etcd/ssl/*.pem

修改/etc/etcd/etcd.conf:
[Member]
#ETCD_CORS=""
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
#ETCD_WAL_DIR=""
ETCD_LISTEN_PEER_URLS="https://192.168.0.131:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.0.131:2379,http://127.0.0.1:2379"
#ETCD_MAX_SNAPSHOTS="5"
#ETCD_MAX_WALS="5"
ETCD_NAME="node1"
#ETCD_SNAPSHOT_COUNT="100000"
#ETCD_HEARTBEAT_INTERVAL="100"
#ETCD_ELECTION_TIMEOUT="1000"
#ETCD_QUOTA_BACKEND_BYTES="0"
#ETCD_MAX_REQUEST_BYTES="1572864"
#ETCD_GRPC_KEEPALIVE_MIN_TIME="5s"
#ETCD_GRPC_KEEPALIVE_INTERVAL="2h0m0s"
#ETCD_GRPC_KEEPALIVE_TIMEOUT="20s"
#
[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.0.131:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.0.131:2379"
#ETCD_DISCOVERY=""
#ETCD_DISCOVERY_FALLBACK="proxy"
#ETCD_DISCOVERY_PROXY=""
#ETCD_DISCOVERY_SRV=""
ETCD_INITIAL_CLUSTER="node1=https://192.168.0.131:2380,node2=https://192.168.0.132:2380,node3=https://192.168.0.234:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-001"
ETCD_INITIAL_CLUSTER_STATE="new"
#ETCD_STRICT_RECONFIG_CHECK="true"
#ETCD_ENABLE_V2="true"
#
#[Proxy]
#ETCD_PROXY="off"
#ETCD_PROXY_FAILURE_WAIT="5000"
#ETCD_PROXY_REFRESH_INTERVAL="30000"
#ETCD_PROXY_DIAL_TIMEOUT="1000"
#ETCD_PROXY_WRITE_TIMEOUT="5000"
#ETCD_PROXY_READ_TIMEOUT="0"
#
[Security]
ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
#ETCD_CLIENT_CERT_AUTH="false"
ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
#ETCD_AUTO_TLS="false"
ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd.pem"
ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
#ETCD_PEER_CLIENT_CERT_AUTH="false"
ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/ca.pem"
#ETCD_PEER_AUTO_TLS="false"
#
#[Logging]
#ETCD_DEBUG="false"
#ETCD_LOG_PACKAGE_LEVELS=""
#ETCD_LOG_OUTPUT="default"
#
#[Unsafe]
#ETCD_FORCE_NEW_CLUSTER="false"
#
#[Version]
#ETCD_VERSION="false"
#ETCD_AUTO_COMPACTION_RETENTION="0"
#
#[Profiling]
#ETCD_ENABLE_PPROF="false"
#ETCD_METRICS="basic"
#
#[Auth]
#ETCD_AUTH_TOKEN="simple"


systemctl start etcd
systemctl enable etcd

验证：
etcdctl --endpoints=https://192.168.0.131:2379,https://192.168.0.132:2379,https://192.168.0.234:2379 --ca-file=/etc/etcd/ssl/ca.pem --cert-file=/etc/etcd/ssl/etcd.pem --key-file=/etc/etcd/ssl/etcd-key.pem cluster-health

-----------------------------------------------------------------------------------------------------------------------------------------


使用kubeadm工具安装的集群，要解除集群的资源占用要先把一些容器停掉，把kube-apiserver的编排文件从/etc/kubernetes/manifests/目录下先移出来，kubelet检查到会停止相应的pods,没有了kube-apiserver集群不会再创建新的pods,这时kubectl不可用了，使用docker命令把spinnaker项目的容器都删掉系统资源就能空闲出来。这时etcd还是正常的，用docker工具直接进入etcd。操作etcd有命令行工具etcdctl，有两个api版本互不兼容的，系统默认的v2版本，kubernetes集群使用的是v3版本，v2版本下是看不到v3版本的数据的:

使用环境变量定义api版本:
export ETCDCTL_API=3
etcd有目录结构类似linux文件系统，获取所有key:
etcdctl get / --prefix --keys-only
--prefix[=false]                  Get keys with matching prefix
--keys-only[=false]               Get only the keys
把想删除的删掉，如:
etcdctl del /registry/deployments/default/elevated-dragonfly-spinn-front50

-----------------------------------------------------------------------------------------------------------------------------------------

安装docker:

所有节点安装配置docker:

yum install -y https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm
yum install -y https://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-17.03.2.ce-1.el7.centos.x86_64.rpm
修改/lib/systemd/system/docker.service：
#ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --registry-mirror=https://ms3cfraz.mirror.aliyuncs.com
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock
systemctl start docker
systemctl enable docker

images="kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0 etcd-amd64:3.1.12 pause-amd64:3.1 kubernetes-dashboard-amd64:v1.8.3 k8s-dns-sidecar-amd64:1.14.8 k8s-dns-kube-dns-amd64:1.14.8 k8s-dns-dnsmasq-nanny-amd64:1.14.8"

注意：
master:
images="kube-proxy-amd64:v1.10.0 kube-scheduler-amd64:v1.10.0 kube-controller-manager-amd64:v1.10.0 kube-apiserver-amd64:v1.10.0 pause-amd64:3.1"
其它node:
images="kube-proxy-amd64:v1.10.0 pause-amd64:3.1"

for imageName in $images;do docker pull keveon/$imageName; docker tag keveon/$imageName k8s.gcr.io/$imageName; docker rmi keveon/$imageName; done

-----------------------------------------------------------------------------------------------------------------------------------------
	
安装kubeadm:

所有节点安装kubelet kubeadm kubectl:

yum install -y kubelet kubeadm kubectl
systemctl enable kubelet

修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf:
Environment="KUBELET_DNS_ARGS=--cluster-dns=10.20.0.10 --cluster-domain=cluster.local"
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"
systemctl daemon-reload

#--cgroup-driver=cgroupfs是为了匹配docker的相关参数
查看docker的Cgroup Driver:
docker info

-----------------------------------------------------------------------------------------------------------------------------------------

所有节点安装命令补全:

yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc

-----------------------------------------------------------------------------------------------------------------------------------------

kubernetes集群

在node1、node2、node3上添加集群初始配置文件:

编辑/etc/kubernetes/config.yaml:
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
etcd:
 endpoints:
  - https://192.168.0.131:2379
  - https://192.168.0.132:2379
  - https://192.168.0.234:2379
 caFile: /etc/etcd/ssl/ca.pem
 certFile: /etc/etcd/ssl/etcd.pem
 keyFile: /etc/etcd/ssl/etcd-key.pem
 dataDir: /var/lib/etcd/default.etcd
networking:
 serviceSubnet: 10.20.0.0/16
 podSubnet: 10.10.0.0/16
kubernetesVersion: 1.10.0
api:
 advertiseAddress: "192.168.0.131"
token: "b99a00.a144ef805368819d"
tokenTTL: "0s"
apiServerCertSANs:
 - node1
 - node2
 - node3
 - 192.168.0.131
 - 192.168.0.132
 - 192.168.0.234
 - 192.168.0.233
featureGates:
 CoreDNS: true
#imageRepository: "registry.cn-hangzhou.aliyuncs.com/k8sth"


首先node1初始化集群:
kubeadm init --config /etc/kubernetes/config.yaml


kubeadm初始化失败后处理办法:
kubeadm reset
#或
rm -rf /etc/kubernetes/*.conf
rm -rf /etc/kubernetes/manifests/*.yaml
docker ps -a |awk '{print $1}' |xargs docker rm -f
systemctl stop kubelet


mkdir -p $HOME/.kube
\cp /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

scp -r /etc/kubernetes/pki root@node2:/etc/kubernetes/
scp -r /etc/kubernetes/pki root@node3:/etc/kubernetes/

部署flannel网络，只需要在node1执行就行:
kubectl apply -f kube-flannel.yml

在node1上依次执行以下命令:

让master也运行pod（默认master不运行pod）:
kubectl taint nodes --all node-role.kubernetes.io/master-

部署dashboard，只需要在node1执行就行:
kubectl apply -f kubernetes-dashboard.yaml

获取token,通过令牌登陆:
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')

在客户端通过浏览器执行:

通过firefox访问dashboard，输入token,即可登陆:
https://192.168.0.131:30000/#!/login

在node1上依次执行以下命令:

安装heapster:
kubectl apply -f kube-heapster/influxdb
kubectl apply -f kube-heapster/rbac

在node2和node3上依次执行以下命令:

kubeadm init --config config.yaml

mkdir -p $HOME/.kube
\cp /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config


添加node4到集群:

在node4上依次执行以下命令:

kubeadm join 192.168.0.131:6443 --token b99a00.a144ef805368819d --discovery-token-ca-cert-hash sha256:7f9d06d7c0a3354a63c1e06bf68d235f267bba76e5a754cd8ac3ecdfee2ebf51

-----------------------------------------------------------------------------------------------------------------------------------------

创建调试pod:
kubectl run curl --image=radial/busyboxplus:curl -i --tty

进入某pod中的第一个容器:
kubectl exec coredns-7997f8864c-hc5xz sh -n kube-system

查看某pod的日志:
kubectl logs heapster-6788866d5f-99qq4 -n kube-system
kubectl logs -f pods/heapster-6788866d5f-99qq4 -n kube-system

-----------------------------------------------------------------------------------------------------------------------------------------


优化:

增加node的可映射端口范围:
修改/etc/kubernetes/manifests/kube-apiserver.yaml:
增加一行:
    - --service-node-port-range=20000-50000

增加node的maxpods:
cat <<END > /etc/kubernetes/kubelet.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
maxPods: 1000
END

修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf:
增加:
Environment="KUBELET_CONFIG=--config=/etc/kubernetes/kubelet.yaml"
修改:
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_CGROUP_ARGS $KUBELET_CERTIFICATE_ARGS $KUBELET_EXTRA_ARGS $KUBELET_CONFIG

systemctl daemon-reload
systemctl restart kubelet


-----------------------------------------------------------------------------------------------------------------------------------------

REST API访问:

方法1:
kubectl proxy模式:
kubectl proxy --port=8080 &
curl http://localhost:8080/api/


方法2:
直接访问:
kubectl config view

TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\t')
curl https://192.168.0.131:6443/api/v1 --header "Authorization: Bearer $TOKEN" -k

-----------------------------------------------------------------------------------------------------------------------------------------

使用etcdctl访问kubernetes数据:

ETCDCTL_API=3 etcdctl get /registry/namespaces/default -w=json|python -m json.tool

使用--prefix可以看到所有的子目录，如查看集群中的namespace:
ETCDCTL_API=3 etcdctl get /registry/namespaces --prefix -w=json|python -m json.tool

key的值是经过base64编码，需要解码后才能看到实际值，如:
echo L3JlZ2lzdHJ5L25hbWVzcGFjZXMvYXV0b21vZGVs|base64 -d

-----------------------------------------------------------------------------------------------------------------------------------------

lvs负载均衡:

k8s集群上:
kubectl run nginx-ds --image=nginx --port=80 --replicas=6 -n work
kubectl expose deployment nginx-ds --port=8888 --protocol=TCP --target-port=80 --name=nginx-svc --external-ip=192.168.0.130 -n work

ipvs服务器上:
IPADDR0=192.168.0.129
NETMASK0=255.255.255.0
IPADDR1=192.168.0.130
NETMASK1=255.255.255.0
GATEWAY0=192.168.0.1

ipvsadm -A -t 192.168.0.130:8888
ipvsadm -a -t 192.168.0.130:8888 -r 192.168.0.131:8888
ipvsadm -a -t 192.168.0.130:8888 -r 192.168.0.132:8888
ipvsadm -a -t 192.168.0.130:8888 -r 192.168.0.233:8888
ipvsadm -a -t 192.168.0.130:8888 -r 192.168.0.234:8888

-----------------------------------------------------------------------------------------------------------------------------------------

ingress负载均衡:

cat << END > nginx-ingress-controller.yaml
---

apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: default-http-backend
  labels:
    app: default-http-backend
  namespace: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: default-http-backend
  template:
    metadata:
      labels:
        app: default-http-backend
    spec:
      terminationGracePeriodSeconds: 60
      containers:
      - name: default-http-backend
        # Any image is permissible as long as:
        # 1. It serves a 404 page at /
        # 2. It serves 200 on a /healthz endpoint
        image: defaultbackend:1.3
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
          requests:
            cpu: 10m
            memory: 20Mi
---

apiVersion: v1
kind: Service
metadata:
  name: default-http-backend
  namespace: ingress-nginx
  labels:
    app: default-http-backend
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: default-http-backend
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
---

kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
        - events
    verbs:
        - create
        - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx
---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ingress-nginx
  template:
    metadata:
      labels:
        app: ingress-nginx
      annotations:
        prometheus.io/port: '10254'
        prometheus.io/scrape: 'true'
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2
          args:
            - /nginx-ingress-controller
            - --default-backend-service=$(POD_NAMESPACE)/default-http-backend
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
                drop:
                - ALL
                add:
                - NET_BIND_SERVICE
            # www-data -> 33
            #runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
          - name: http
            containerPort: 80
          - name: https
            containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
END

kubectl create -f nginx-ingress-controller.yaml

测试示例:

kubectl run nginx-ds-1 --image=nginx --port=80 --replicas=5 -n ingress-nginx --image-pull-policy='IfNotPresent'
kubectl expose deployment nginx-ds-1 --port=80 --protocol=TCP --target-port=80 --name=nginx-svc-1 --type='ClusterIP' -n ingress-nginx

cat << END > nginx-ingress-test.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-ingress-test
  namespace: ingress-nginx
spec:
  rules:
  - host: www.test.com
    http:
      paths:
      - backend:
          serviceName: nginx-svc-1
          servicePort: 80
END

kubectl create -f nginx-ingress-test.yaml

在测试客户机上:
echo "nginx-ingress-controller这个pod所在node的IP地址		www.test.com" >> /etc/hosts
curl -v www.test.com

